<!-- after experiments, 400px for single image, 300px for two aligning imges, 200px for three -->
<html>
	<head>
		<style>
			@media print {
				body {
					margin: 0;
					width: 100%;
				}

				img {
					max-width: 100%;
					height: auto;
				}

				figure {
					page-break-inside: avoid;
				}

				/* 避免元素被缩窄 */
				.no-shrink {
					width: 100% !important;
					display: block !important;
				}

				/* 如果你有 flex 布局，改成 block，避免被压缩 */
				.flex-wrapper {
					display: block !important;
				}
			}
		</style>

		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
	<div class="flex-wrapper no-shrink" style="display: flex; justify-content: center; gap: 20px; margin-top: 20px;">
	<h1>CS184 Summer 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Names (with no order): Xiangru Huang (3042029019), Yanshi Liang (3041912864)</div>

		<br>

		Link to webpage (and images): <a href="https://github.com/cal-cs184/hw-webpages-su25-Echoooggu/blob/main/hw3/index.html">
			github.com/cal-cs184/hw-webpages-su25-Echoooggu/blob/main/hw3/index.html</a><br>
		Link to GitHub repository (codes): <a href="https://github.com/cal-cs184/hw-pathtracer-updated-fishros_version3">
			github.com/cal-cs184/hw-pathtracer-updated-fishros_version3</a>


		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
			<p>
				Basically, we implemented how to render a scene by mainly using Monte Carlo Sampling for path tracing.
				Starting from generating camera rays, we calculate the intersections of the rays and the objects,
				and compute the illuminance (direct first, and then indirect, and then sum them up to get global).
				Along the journey, simple visualization is not enough.
				We tried different algorithms to speed up the rendering process, or/and improve the quality of the image.
				These include BVH (Bounding Volume Hierarchy), Importance Sampling, Russian Roulette, and Adaptive Sampling.
			</p>
			<p>
				Problems encountered: Part3 and Part4 are especially difficult to debug. Part5 also took a few hours.
				At last we solved our problems with the help of TAs.
				We learned that sometimes the problems are not with the algorithms or major part of the codes,
				but really tiny details that we missed or misunderstood.
			</p>
		    <p>
				Acknowledgement of AI:<br>
				1. Part3Task4: Our rendered images looked a bit dimmer than the reference images, so we asked ChatGPT for help.
				But GPT couldn't solve it, nor could Cursor or Claude, so eventually we went to OH and found that our EPS_F was offset.<br>
				2. Part4Task2: We found that our categorization was a bit messy.
				Initially, we categorized the cases into: first judge whether the rays can hit another object,
				and then categorized based on isAccumBounces.
				But it turned out that it'd be better to think about if we first categorize using isAccumBounces, which is what we learned by using Claude.
				Then we reorganized our codes.<br>
				3. Part4Task3: We didn't really understand Russian Roulette, so we used Claude to update the codes from task2.
				Then, we managed to understand it, also using the codes to understand knowledge we learned in slides, and then replicated it.
			</p>


		<h2>Part 1: Ray Generation and Scene Intersection</h2>
			<p>Results of Part1:</p>
			<div style="display: flex; flex-direction: column; align-items: center;">
				<table style="width: 100%; text-align: center; border-collapse: collapse;">
					<tr>
						<td style="text-align: center;">
							<img src="images/CBempty_part1task3.png" width="300px"/>
							<figcaption>Part1 after implementing ray-triangle intersection (CBempty.dae)</figcaption>
						</td>
						<td style="text-align: center;">
							<img src="images/CBspheres_part1task4.png" width="300px"/>
							<figcaption>Part1 after implementing ray-sphere intersection (CBspheres_lambertian.dae)</figcaption>
						</td>
					</tr>
				</table>
			</div>
		<h3>1. Ray generation:</h3>
			<p>
				In ray tracing, rendering starts by shooting rays from the camera into the scene to determine what is visible at each pixel.
				For each pixel on the image plane, we compute normalized coordinates (x, y) representing the pixel’s location on the camera’s image sensor, normalized to [0,1].
				These normalized coordinates are then transformed into coordinates on the camera’s viewing plane which maps to physical locations in the camera’s coordinate system.
				Using the camera’s parameters such as field of view and orientation, we construct a ray starting from the camera position, pointing through that pixel’s location on the viewing plane.
				This ray is represented by an origin point and a direction vector.
				The key purpose here is to translate 2D pixel positions into 3D rays that simulate light paths coming from the camera into the scene.
			</p>

		<h3>2. Primitive intersection:</h3>
			<p>
				For triangles, we implemented the Möller–Trumbore intersection algorithm.
				This method transforms the intersection problem into a system of linear equations
				by expressing the intersection point as a combination of triangle edges and solving for barycentric coordinates.
				If these coordinates fall within the triangle (u ≥ 0, v ≥ 0, and u + v ≤ 1),
				and the intersection t lies within the valid ray segment ([min_t, max_t]),
				then the triangle is considered hit. Upon a valid hit, the ray’s max_t is updated to the nearest intersection point,
				helping to cull farther intersections during traversal.
				For spheres, the intersection test solves the quadratic equation derived
				from substituting the ray equation into the implicit equation of a sphere.
				The two roots represent the possible intersection points.
				We select the valid root that lies within the ray's bounds and update the ray accordingly.
				If an intersection is found, we record the hit time,
				surface normal (from sphere center to the hit point),
				and material information.
			</p>

		<h3>3. Triangle intersection algorithm:</h3>
			<p>
				In our implementation, we used the Möller–Trumbore algorithm to determine whether a given ray intersects a triangle.
				The process begins by computing two edge vectors of the triangle based on its three vertices.
				Then, we calculate a vector that is perpendicular to the ray direction and one of the triangle's edges.
				This helps compute a value called the determinant, which tells us whether the ray and the triangle are nearly parallel.
				If the determinant is close to zero, there's no intersection.
				If the determinant is non-zero, we proceed to compute the u and v barycentric coordinates of the intersection point.
				These coordinates tell us whether the intersection point lies inside the triangle.
				If u < 0, v < 0, or u + v > 1, then the point is outside the triangle, and we reject it.
				Finally, we calculate the t-value (how far along the ray the intersection occurs).
				If t falls within the valid range defined by the ray (between min_t and max_t),
				we confirm an intersection.
				The ray's maximum t is updated so that future intersections only consider closer objects.
			</p>

		<h3>Here are also some .dae files with normal shading: (which are also rendered in Part2, but with a much slower rendering speed)</h3>
			<div style="display: flex; flex-direction: column; align-items: center;">
				<table style="width: 100%; text-align: center; border-collapse: collapse;">
					<tr>
						<td style="text-align: center;">
							<img src="images/cow_part2.png" width="200px"/>
							<figcaption>cow.dae</figcaption>
						</td>
						<td style="text-align: center;">
							<img src="images/maxplanck_part2.png" width="200px"/>
							<figcaption>maxplanck.dae</figcaption>
						</td>
						<td style="text-align: center;">
							<img src="images/CBlucy_part2.png" width="200px"/>
							<figcaption>CBlucy.dae</figcaption>
						</td>
					</tr>
				</table>
			</div>

		
		<h2>Part 2: Bounding Volume Hierarchy</h2>
		<h3>1. BVH Construction and Splitting Heuristic</h3>
			<p>
				In our implementation of construct_bvh,
				we recursively construct the BVH tree using the primitives provided in the input.
				The process begins by computing a bounding box that encloses all primitives in the current range.
				This bounding box determines the spatial extent of the current BVH node.
			</p>
			<p>
				To decide whether the current node is a leaf,
				we use two criteria.
				The first one is the number of primitives in the range is less than or equal to max_leaf_size,
				another is that all primitives share the same centroid along the longest axis .
				If either condition is met,
				we construct a leaf node by storing pointers to the primitives.
			</p>
			<p>
				Otherwise, we proceed to split the primitives into two halves.
				To determine the splitting axis,
				we calculate the extent of the bounding box along the X, Y, and Z axes and select the axis with the largest extent.
				This ensures that we divide along the most "spread-out" direction,
				which helps improve spatial partitioning.
			</p>
			<p>
				After selecting the splitting axis based on the longest extent of the bounding box,
				we organize the primitives according to the position of their bounding box centroids along this axis.
				Specifically, we sort the primitives so that those with smaller centroid coordinates along the chosen axis come before those with larger coordinates.
				This ordering helps us to spatially partition the primitives into two balanced groups.
			<p>
			<p>
			Instead of simply splitting at the average centroid value,
			we choose the splitting point as the median centroid coordinate,
			corresponding to the primitive at the middle index of the sorted list.
			</p>
			<p>
				Finally, we recursively build the left and right child nodes using the two halves of the primitive list,
				and return a BVHNode that represents the internal node with bounding box and child pointers.
			</p>

		<h3>2. Impact of BVH Acceleration on Rendering Performance</h3>
			<p>
				Using BVH acceleration significantly improves rendering performance,
				especially for scenes with moderately complex geometries.
				For instance, in the "cow" scene,
				rendering without BVH took 1.65 seconds,
				while with BVH it only took 0.0256 seconds — a speedup of over 64 times.
				In a second scene,
				rendering time dropped from 18.4412 seconds without BVH to just 0.0343 seconds with BVH,
				showing a speedup of more than 500imes.
				Similarly, in a third scene,
				the BVH-accelerated rendering completed in 0.0278 seconds compared to 123.8051 seconds without BVH — a staggering improvement of over 4,000times.
				These results clearly demonstrate the effectiveness of BVH in reducing the number of intersection tests and accelerating the rendering process.
			</p>
			<div style="display: flex; flex-direction: column; align-items: center;">
				<table style="width: 100%; text-align: center; border-collapse: collapse;">
					<tr>
						<td style="text-align: center;">
							<img src="images/cow_part2.png" width="200px"/>
							<figcaption>cow.dae</figcaption>
						</td>
						<td style="text-align: center;">
							<img src="images/cow_without_bvh.jpg" width="200px"/>
							<figcaption>cow.dae without BVH: 1.6520s</figcaption>
						</td>
						<td style="text-align: center;">
							<img src="images/cow_with_bvh.jpg" width="200px"/>
							<figcaption>cow.dae with BVH: 0.0256s</figcaption>
						</td>
					</tr>
				</table>
			</div>
			<div style="display: flex; flex-direction: column; align-items: center;">
				<table style="width: 100%; text-align: center; border-collapse: collapse;">
					<tr>
						<td style="text-align: center;">
							<img src="images/maxplanck_part2.png" width="200px"/>
							<figcaption>maxplanck.dae</figcaption>
						</td>
						<td style="text-align: center;">
							<img src="images/maxplanck_without%20bvh.jpg" width="200px"/>
							<figcaption>maxplanck.dae without BVH: 18.4412s</figcaption>
						</td>
						<td style="text-align: center;">
							<img src="images/maxplanck_with_bvh.jpg" width="200px"/>
							<figcaption>maxplanck.dae with BVH: 0.0343s</figcaption>
						</td>
					</tr>
				</table>
			</div>
			<div style="display: flex; flex-direction: column; align-items: center;">
				<table style="width: 100%; text-align: center; border-collapse: collapse;">
					<tr>
						<td style="text-align: center;">
							<img src="images/CBlucy_part2.png" width="200px"/>
							<figcaption>CBlucy.dae</figcaption>
						</td>
						<td style="text-align: center;">
							<img src="images/CBlucy_without%20bvh.jpg" width="200px"/>
							<figcaption>CBlucy.dae without BVH: 123.8051s</figcaption>
						</td>
						<td style="text-align: center;">
							<img src="images/CBlucy_with_bvh.jpg" width="200px"/>
							<figcaption>CBlucy.dae with BVH: 0.0278s</figcaption>
						</td>
					</tr>
				</table>
			</div>


		<h2>Part 3: Direct Illumination</h2>
			<h3>1. Implementation Overview of Direct Lighting Functions</h3>
			<h4>a. <code>Vector3D PathTracer::estimate_direct_lighting_hemisphere</code></h4>
				<p>
					In our implementation of <code>estimate_direct_lighting_hemisphere</code>,
					we estimate direct lighting by uniformly sampling directions over the hemisphere centered around the surface normal at the intersection point.
					This approach follows a Monte Carlo integration strategy,
					where we randomly sample directions and accumulate radiance contributions from light-emitting surfaces that are visible along those directions.
				</p>
			<p>
				We begin by constructing a local coordinate system at the intersection point such that the Z-axis aligns with the surface normal.
				This coordinate frame allows us to convert both the incoming and outgoing directions between world space and local shading space,
				where BSDF evaluation is more convenient.
				We then compute the hit point and outgoing direction in the local coordinate system.
			</p>
			<p>
				We determine the number of samples by multiplying the number of lights in the scene by the number of area light samples (ns_area_light).
				For each sample, we randomly generate a direction w_in over the unit hemisphere using uniform sampling.
				This direction is then transformed into world space, and a shadow ray is cast from the surface point toward this direction.
				If the shadow ray intersects an object and the intersected object is emissive, we treat it as a contribution to direct illumination.
			</p>
			<p>
				The radiance contribution for each valid sample is calculated using the rendering equation,
				including the surface BSDF, the emitted light from the intersected point,
				and the cosine of the angle between the incoming direction and the surface normal.
				We accumulate these contributions over all samples.
			</p>
			<p>
				Finally, since we are using uniform sampling over the hemisphere,
				we multiply the average contribution by 2π to account for the solid angle of the hemisphere.
				This results in an unbiased estimate of the direct illumination at the surface point.
			</p>

			<h4>b. <code>Vector3D PathTracer::estimate_direct_lighting_importance</code></h4>
				<p>
					At each intersection,
					we construct a local coordinate frame aligned with the surface normal to simplify calculations in the shading space.
					We then iterate over all lights in the scene.
					For delta lights, such as point or directional lights,
					we take a single sample since they emit light from a single direction.
					For area lights, we take multiple samples.
					For each light sample,
					we obtain a direction toward the light and an associated radiance value and probability density.
					We then perform a shadow ray test to check for occlusion:
					if another object blocks the path to the light,
					we discard the sample.
					If the path is clear,
					we evaluate the surface BSDF to determine how much of the incoming radiance is reflected toward the camera,
					and we scale the contribution by the cosine of the incident angle and the inverse of the sampling PDF.
					These contributions are accumulated and averaged over all valid samples to produce the final estimate of direct lighting.
				</p>

			<h3>2. Visual Results: Hemisphere vs. Importance Sampling</h3>
			<p>Shadow quality:</p>
			<p>
				Hemisphere Sampling produces noisy soft shadows, especially under area lights. The shadows tend to be grainy unless a high number of samples are used.<br>
				Importance Sampling yields smoother, more realistic soft shadows even with fewer samples.
			</p>
			<p>Rendering efficiency:</p>
			<p>
				For Hemisphere Sampling, many samples are wasted in directions that contribute no direct lighting.<br>
				However, for Importance Sampling, nearly all samples are useful, leading to better results per sample.
			</p>
			<div style="display: flex; justify-content: center; gap: 20px; margin-top: 20px;">
				<figure>
					<img src="images/CBbunny_H_64_32part3task3.png" alt="Task 1 result" style="width:300px">
					<figcaption>Part3: hemisphere sampling <br>
						(CBbunny.dae, -s 64, -l 32, -m 6)</figcaption>
				</figure>
				<figure>
					<img src="images/bunny_64_32_part3task4.png" style="width: 300px;">
					<figcaption>Part3: importance sampling <br>
						(CBbunny.dae, -s 64, -l 32, -m 6)</figcaption>
				</figure>
			</div>

			<h3>3. Noise Analysis in Soft Shadows with Varying Light Samples (importance sampling)</h3>
			<p>
				The results below show a clear trend:
				as the number of light rays increases, the soft shadows become noticeably smoother and less noisy.
				With only 1 light sample, the shadows appear extremely grainy and unstable.
				At 4 samples, the noise is slightly reduced but still visually distracting.
				With 16 light rays,much of the high-frequency noise is suppressed.
				Finally, at 64 light rays, the shadows are significantly cleaner,
				with soft penumbra transitions and minimal noise.
				This confirms that increasing the number of light samples improves the estimation of area light contributions
				and leads to more realistic and stable shadow rendering, even with just one pixel sample.
			</p>
			<div style="display: flex; flex-direction: column; align-items: center;">
				<table style="width: 100%; text-align: center; border-collapse: collapse;">
					<tr>
						<td style="text-align: center;">
							<img src="images/bunny_1_1_part3task4.png" width="300px"/>
							<figcaption>CBbunny.dae, -s 1, -l 1</figcaption>
						</td>
						<td style="text-align: center;">
							<img src="images/bunny_1_4_part3task4.png" width="300px"/>
							<figcaption>CBbunny.dae, -s 1, -l 4</figcaption>
						</td>
					</tr>
					<tr>
						<td style="text-align: center;">
							<img src="images/bunny_1_16_part3task4.png" width="300px"/>
							<figcaption>CBbunny.dae, -s 1, -l 16</figcaption>
						</td>
						<td style="text-align: center;">
							<img src="images/bunny_1_64_part3task4.png" width="300px"/>
							<figcaption>CBbunny.dae, -s 1, -l 64</figcaption>
						</td>
					</tr>
				</table>
			</div>
			<p>
				ps. other images mentioned in the spec:
			</p>
			<div style="display: flex; flex-direction: column; align-items: center;">
				<table style="width: 100%; text-align: center; border-collapse: collapse;">
					<tr>
						<td style="text-align: center;">
							<img src="images/CBbunny_16_8_part3task2.png" width="200px"/>
							<figcaption>zero-bounce radiance<br>
								(CBbunny.dae, -s 16, -l 8)
							</figcaption>
						</td>
						<td style="text-align: center;">
							<img src="images/CBbunny_H_16_8_part3task3.png" width="200px"/>
							<figcaption>using hemisphere sampling <br>
								(CBbunny.dae, -s 16, -l 8)</figcaption>
						</td>
						<td style="text-align: center;">
							<img src="images/dragon_64_32_part3task4.png" width="200px"/>
							<figcaption>using importance sampling <br>
								(CBdragon.dae, -s 64, -l 32, -m 6)</figcaption>
						</td>
					</tr>
				</table>
			</div>


		<h2>Part 4: Global Illumination</h2>
			<h3>1. Implementation of the indirect lighting function  (<code>mainly at_least_one_bounce_radiance(…)</code>) </h3>
			<p>
				a.	Construct an orthonormal basis (o2w) and take its transpose (w2o)
			</p>
			<p>
				b.	Sample an incoming light direction w_in based on the outgoing direction w_out,
				and calculate the BSDF value for this pair.
			</p>
			<p>
				c.	Set up a new ray for indirect bounce (The sampled w_in in object space is transformed into world coordinates.)
				(details: use EPS_F to avoid self-intersection)
			</p>
			<p>
				d.	Accumulated bounces: <br>
				(i) if the new ray hits another object,
				then recursively compute incoming radiance,
				weight it by the BSDF f, cos_theta, and divide by pdf.
				Here we get indirect_contribution.
				If Russian Roulette is applied, divide further by the continuation probability to maintain unbiasedness. <br>
				(ii) If the ray hits no object, but an environment light exists,
				then sample the light directly in the direction of the ray.
				Same BSDF and weighting logic applies and we get indirect_contribution.
				In both cases, indirect_contribution is accumulated to L_out.
			</p>
			<p>
				e.	Unaccumulated bounces: <br>
				only the current bounce level is evaluated.
				The formula for calculating indirect_contribution is still the same (see below image).
			</p>
			<figure>
				<img src="images/the_reflection_equation.jpg" style="width: 400px;">
				<figcaption>the reflection equation</figcaption>
			</figure>


			<h3>2. Bunny rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.</h3>
			 	<figure>
					<img src="images/bunny_s1024_m4_true_part4task2.png" width=" 200px">
					<figcaption>Part4: global illuminance (bunny -s 1024 -m 4)</figcaption>
				</figure>

			<h3>3. Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. </h3>
			<p>
				Only direct illumination (i.e. zero-bounce && one-bounce): looks more realistic and brighter than images with only indirect illumination. <br>
				Only indirect illumination (i.e. ! zero-bounce && one-bounce): looks much dimmer as if it's fading.
				However, if we compare this image with the one with only the m-th bounce (m > 1), this one is brighter as it is the sum.
				And if we compare this image with the one with only 2nd bounce,
				shadows in this image are less distinct as the m-th bounces (m > 2) smooth it out.
			</p>
			<div style="display: flex; justify-content: center; gap: 20px; margin-top: 20px;">
				<figure>
					<img src="images/bunny_with_only_direct_lighting_part4.png" alt="Task 1 result" style="width:300px">
					<figcaption>only direct illumination</figcaption>
				</figure>
				<figure>
					<img src="images/bunny_part4_only_indirect.png" style="width: 300px;">
					<figcaption>only indirect illumination</figcaption>
				</figure>
			</div>

			<h3>4. CBbunny.dae with max_ray_depth set to 0, 1, 2, 3, 4, and 5 (the -m flag), isAccumBounces=false, -s 1024 </h3>
			<p>
				(Explain in your write-up what you see for the 2nd and 3rd bounce of light,
				and how it contributes to the quality of the rendered image compared to rasterization.)<br>
			</p>
			<p>a. What We See for the 2nd and 3rd Bounce of Light (with isAccumBounces = false):</p>
			<p>
				(i) 2nd bounce: <br>
				Much darker than the 1st bounce of light, as if the image is fading.
				Direct lighting (including emission and one-bounce) are missing.
				Shadows are less distinct from the lighting regions.
				The light looks gentler and smoother. <br>
				(ii) 3rd bounce: <br>
				Even dimmer than the 2nd bounce of light. Direct lighting is still missing.
				There’s no obvious shadows because the whole scene is dark. Light looks even smoother.
			</p>
			<div style="display: flex; justify-content: center; gap: 20px; margin-top: 20px;">
				<figure>
					<img src="images/bunny_s1024_m2_false_part4task2.png" style="width:300px">
					<figcaption>only 2nd bounce</figcaption>
				</figure>
				<figure>
					<img src="images/bunny_s1024_m3_false_part4task2.png" style="width: 300px;">
					<figcaption>only 3rd bounce</figcaption>
				</figure>
			</div>
			<p>b. How it contributes to the quality of the rendered image compared to rasterization: </p>
			<p>
				Rasterization typically only includes direct lighting and uses hacks (like ambient occlusion) to fake indirect illumination.
				Isolated 2nd and 3rd bounces expose how indirect light alone behaves.
			</p>
			<p>c. Compare rendered views of accumulated and unaccumulated bounces for CBbunny.dae with max_ray_depth set to 0, 1, 2, 3, 4, and 5 (the -m flag). Use 1024 samples per pixel.</p>
			<p>
				(i) Accumulated: <br>
				In a full rendering (with accumulated bounces),
				every layer of bounces adds soft shadows, light bleeding, and fill light into areas that would otherwise be too dark.
				As we add on the maximum ray depth (i.e. the -m flag), the view becomes brighter as there are more lighting adding on. <br>
				(ii) Unaccumulated: <br>
				Radiance decreases as the number of bounces increases.
				Thus, as we add on the maximum ray depth (i.e. the -m flag),
				the view becomes dimmer and dimmer.
			</p>
			<table style="border-collapse: collapse;">
				<tr>
					<th style="border: 1px solid black; padding: 10px;"></th>
					<th style="border: 1px solid black; padding: 10px;">m = 0</th>
					<th style="border: 1px solid black; padding: 10px;">m = 1</th>
					<th style="border: 1px solid black; padding: 10px;">m = 2</th>
					<th style="border: 1px solid black; padding: 10px;">m = 3</th>
					<th style="border: 1px solid black; padding: 10px;">m = 4</th>
					<th style="border: 1px solid black; padding: 10px;">m = 5</th>
				</tr>
				<tr>
					<th style="border: 1px solid black; padding: 10px;">isAccumBounces = false</th>
					<td style="border: 1px solid black; padding: 10px;"><img src="images/bunny_s1024_m0_false_part4task2.png" width="100"></td>
					<td style="border: 1px solid black; padding: 10px;"><img src="images/bunny_s1024_m1_false_part4task2.png" width="100"></td>
					<td style="border: 1px solid black; padding: 10px;"><img src="images/bunny_s1024_m2_false_part4task2.png" width="100"></td>
					<td style="border: 1px solid black; padding: 10px;"><img src="images/bunny_s1024_m3_false_part4task2.png" width="100"></td>
					<td style="border: 1px solid black; padding: 10px;"><img src="images/bunny_s1024_m4_false_part4task2.png" width="100"></td>
					<td style="border: 1px solid black; padding: 10px;"><img src="images/bunny_s1024_m5_false_part4task2.png" width="100"></td>
				</tr>
				<tr>
					<th style="border: 1px solid black; padding: 10px;">isAccumBounces = true</th>
					<td style="border: 1px solid black; padding: 10px;"><img src="images/bunny_s1024_m0_true_part4task2.png" width="100"></td>
					<td style="border: 1px solid black; padding: 10px;"><img src="images/bunny_s1024_m1_true_part4task2.png" width="100"></td>
					<td style="border: 1px solid black; padding: 10px;"><img src="images/bunny_s1024_m2_true_part4task2.png" width="100"></td>
					<td style="border: 1px solid black; padding: 10px;"><img src="images/bunny_s1024_m3_true_part4task2.png" width="100"></td>
					<td style="border: 1px solid black; padding: 10px;"><img src="images/bunny_s1024_m4_true_part4task2.png" width="100"></td>
					<td style="border: 1px solid black; padding: 10px;"><img src="images/bunny_s1024_m5_true_part4task2.png" width="100"></td>
				</tr>
			</table>
			<h3>5. For CBbunny.dae, output the Russian Roulette rendering with max_ray_depth set to 0, 1, 2, 3, 4, and 100(the -m flag). Use 1024 samples per pixel.</h3>
			<div style="display: flex; flex-direction: column; align-items: center;">
				<table style="width: 100%; text-align: center; border-collapse: collapse;">
					<tr>
						<td style="text-align: center;">
							<img src="images/bunny_with_rr_s1024_m0_part4task3.png" width="300px"/>
							<figcaption>Russian Roulette, -s 1024, -m 0</figcaption>
						</td>
						<td style="text-align: center;">
							<img src="images/bunny_with_rr_s1024_m1_part4task3.png" width="300px"/>
							<figcaption>Russian Roulette, -s 1024, -m 1</figcaption>
						</td>
					</tr>
					<tr>
						<td style="text-align: center;">
							<img src="images/bunny_with_rr_s1024_m2_part4task3.png" width="300px"/>
							<figcaption>Russian Roulette, -s 1024, -m 2</figcaption>
						</td>
						<td style="text-align: center;">
							<img src="images/bunny_with_rr_s1024_m3_part4task3.png" width="300px"/>
							<figcaption>Russian Roulette, -s 1024, -m 3</figcaption>
						</td>
					</tr>
					<tr>
						<td style="text-align: center;">
							<img src="images/bunny_with_rr_s1024_m4_part4task3.png" width="300px"/>
							<figcaption>Russian Roulette, -s 1024, -m 4</figcaption>
						</td>
						<td style="text-align: center;">
							<img src="images/bunny_with_rr_s1024_m100_part4task3.png" width="300px"/>
							<figcaption>Russian Roulette, -s 1024, -m 100</figcaption>
						</td>
					</tr>
				</table>
			</div>
			<p>ps. other images using Raussian Roulette mentioned in the spec:</p>
			<div style="display: flex; justify-content: center; gap: 20px; margin-top: 20px;">
				<figure>
					<img src="images/spheres_with_rr_part4task3.png" style="width:300px">
				</figure>
				<figure>
					<img src="images/dragon_with_rr_part4task3.png" style="width: 300px;">
				</figure>
			</div>

			<h3>6. Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays. (here we used -m to be 1)</h3>
			<p>
				As sample-per-pixel rates increase, the images become less noisy and coarse, and also become smoother.
			</p>
			<div style="display: flex; justify-content: center; gap: 10px; margin-top: 10px;">
				<figure>
					<img src="images/bunny_s1_l4_part4.png" width="200px">
					<figcaption>Part4: -s 1, -l 4, -m 1</figcaption>
				</figure>
				<figure>
					<img src="images/bunny_s2_l4_part4.png" width="200px">
					<figcaption>Part4: -s 2, -l 4, -m 1</figcaption>
				</figure>
				<figure>
					<img src="images/bunny_s4_l4_part4.png" width="200px">
					<figcaption>Part4: -s 4, -l 4, -m 1</figcaption>
				</figure>
			</div>
			<div style="display: flex; justify-content: center; gap: 10px; margin-top: 10px;">
				<figure>
					<img src="images/bunny_s8_l4_part4.png" width="200px">
					<figcaption>Part4: -s 8, -l 4, -m 1</figcaption>
				</figure>
				<figure>
					<img src="images/bunny_s16_l4_part4.png" width="200px">
					<figcaption>Part4: -s 16, -l 4, -m 1</figcaption>
				</figure>
				<figure>
					<img src="images/bunny_s64_l4_part4.png" width="200px">
					<figcaption>Part4: -s 64, -l 4, -m 1</figcaption>
				</figure>
				<figure>
					<img src="images/bunny_s1024_l4_part4.png" width="200px">
					<figcaption>Part4: -s 1024, -l 4, -m 1</figcaption>
				</figure>
			</div>

			<h2>Part 5: Adaptive Sampling</h2>
			<h3>1. Explanation of Adaptive Sampling:</h3>
			<p>
				Adaptive sampling, utilizing confidence interval, is a technique in ray tracing that dynamically decides how many samples are needed per pixel,
				instead of always taking a fixed number.
				The idea is to reduce computation time while preserving image quality by stopping early on pixels that have already converged (i.e., their color estimate is stable enough).
			</p>
			<p>2. Our Implementation of Adaptive Sampling: </p>
			<p>
				a. Initialization: <br>
				For each pixel (x, y), we initialize accumulators for the pixel colors and for computing the statistics of illuminance.
				Specifically, we track the sum of illuminance values, the squared sum of illuminance values, and the number of actual samples
				to calculate the mean and variance of the sampled sub-pixel illuminance values later.
			</p>
			<p>
				b. Sampling loop: <br>
				We loop up to a predefined maximum number of samples (max_samples). For each sample:
				We generate a subpixel sample using gridSampler.
				We compute a ray from the camera using that subpixel location.
				We trace the ray through the scene using <code>est_radiance_global_illumination(ray)</code>.
				The returned radiance contributes to the pixel's color estimate.
				We compute the illuminance (brightness) of the radiance and update s1 and s2.
			</p>
			<p>
				c. Confidence check:<br>
				After each sample, we compute:
				The mean illuminance mu
				The sample variance
				The standard deviation sigma
				The confidence interval width I, using a 95% confidence interval approximation (see the formula below).
				If this interval I is smaller than a user-defined tolerance maxTolerance × mu,
				then we consider the pixel “converged” and break early from the loop (i.e. don't sample anymore).
			</p>
			<figure>
				<img src="images/confidence_interval_formula.jpg" width="400px">
				<figcaption>confidence interval formula</figcaption>
			</figure>
			<p>
				d. Finalize pixel value:<br>
				Once sampling finishes (either by convergence or reaching the maximum sample count),
				we average the total color estimate by the actual number of samples taken and update the sample and color buffers.
			</p>

			<h3>Two scenes with -s 2048, -l 1, -m 5:</h3>
			<div style="display: flex; justify-content: center; gap: 20px; margin-top: 20px;">
				<figure>
					<img src="images/bunny_part5_s2048_m5.png" alt="Task 1 result" style="width:300px">
					<figcaption>rendered image1: bunny</figcaption>
				</figure>
				<figure>
					<img src="images/bunny_part5_s2048_m5_rate.png" style="width: 300px;">
					<figcaption>sample rate image1</figcaption>
				</figure>
			</div>
			<div style="display: flex; justify-content: center; gap: 20px; margin-top: 20px;">
				<figure>
					<img src="images/spheres_lambertian_part5_s2048_m5.png" alt="Task 1 result" style="width:300px">
					<figcaption>rendered image2: spheres_lambertian</figcaption>
				</figure>
				<figure>
					<img src="images/spheres_lambertian_part5_s2048_m5_rate.png" style="width: 300px;">
					<figcaption>sample rate image2</figcaption>
				</figure>
			</div>

			<h3>THE END</h3>
			<p>
				This html is edited by Xiangru Huang, so here I use 'I':)<br>
				I'm really glad we found a partner—it’s like having a second brain,
				which has made both work and study much more efficient.
				Especially when we don’t have enough time to review before exams, we divide up the slides and teach each other.
				This way, we use the Feynman learning method to solidify our own understanding
				while also helping the other person quickly grasp the content of the slides.
				We also split up the homeworks. After completing each part,
				we explain to each other the implementation ideas and key details to watch out for.
				This has made the heavy workload of this course a bit more manageable.
				More importantly, as exchange students here, we don’t know many people,
				and this course is particularly challenging for us.
				So we’re not just studying together, but supporting each other like we’re “in the same boat.”
				We’re both from the same university in mainland China, and we used to just casually greet each other.
				Now, we’ve become much closer. I’m truly happy we’ve been able to collaborate,
				share our excitement and anxiety, and learn from each other!
			</p>
		</div>
	</body>
</html>